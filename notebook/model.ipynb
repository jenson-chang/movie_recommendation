{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from lightfm import LightFM\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from lightfm.evaluation import precision_at_k, auc_score\n",
    "import pickle\n",
    "import os\n",
    "from scipy.sparse import save_npz, load_npz, identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from MovieLens 32M dataset https://grouplens.org/datasets/movielens/100k\n",
    "ratings_df = pd.read_csv('data/ratings.csv')\n",
    "links_df = pd.read_csv('data/links.csv')\n",
    "movies_df = pd.read_csv('data/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert ratings to use TMDB IDs\n",
    "ratings_df = ratings_df.merge(links_df[['movieId', 'tmdbId']], on='movieId', how='inner')\n",
    "\n",
    "# # Rename columns to match the expected format\n",
    "ratings_df = ratings_df.drop(columns=['movieId', 'timestamp'], axis=1)\n",
    "ratings_df = ratings_df.rename(columns={'tmdbId': 'movieId'})\n",
    "\n",
    "# Remove any rows with NaN values\n",
    "ratings_df = ratings_df.dropna(subset=['userId', 'movieId', 'rating'])\n",
    "\n",
    "# Ensure userId and movieId are integers\n",
    "ratings_df['userId'] = ratings_df['userId'].astype(int)\n",
    "ratings_df['movieId'] = ratings_df['movieId'].astype(int)\n",
    "\n",
    "# Binarize ratings\n",
    "ratings_df['rating'] = (ratings_df['rating'] >= 4.0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse matrix shape: (610, 9715)\n",
      "Matrix density: 1.7013%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create mappings for user and movie IDs\n",
    "user_ids = ratings_df['userId'].unique()\n",
    "movie_ids = ratings_df['movieId'].unique()\n",
    "\n",
    "user_id_map = {id_: i for i, id_ in enumerate(user_ids)}\n",
    "movie_id_map = {id_: i for i, id_ in enumerate(movie_ids)}\n",
    "\n",
    "# Convert ratings to matrix format\n",
    "row_ind = [user_id_map[x] for x in ratings_df['userId']]\n",
    "col_ind = [movie_id_map[x] for x in ratings_df['movieId']]\n",
    "ratings = ratings_df['rating'].values\n",
    "\n",
    "# Create sparse matrix\n",
    "sparse_matrix = csr_matrix(\n",
    "    (ratings, (row_ind, col_ind)),\n",
    "    shape=(len(user_ids), len(movie_ids))\n",
    ")\n",
    "\n",
    "print(\"Sparse matrix shape:\", sparse_matrix.shape)\n",
    "print(\"Matrix density: {:.4f}%\".format(\n",
    "    100 * sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1])\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training matrix shape: (610, 9715)\n",
      "Testing matrix shape: (610, 9715)\n"
     ]
    }
   ],
   "source": [
    "def leave_one_out_split(ratings_df):\n",
    "    \"\"\"\n",
    "    Perform a leave-one-out split: for each user, hold out 1 interaction for testing.\n",
    "    \"\"\"\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    \n",
    "    grouped = ratings_df.groupby('userId')\n",
    "    \n",
    "    for user_id, group in grouped:\n",
    "        if len(group) < 2:\n",
    "            # If user has only 1 interaction, keep it in training\n",
    "            train_list.append(group)\n",
    "        else:\n",
    "            # Randomly sample one for test\n",
    "            test_sample = group.sample(n=1, random_state=42)\n",
    "            train_sample = group.drop(test_sample.index)\n",
    "            \n",
    "            test_list.append(test_sample)\n",
    "            train_list.append(train_sample)\n",
    "    \n",
    "    ratings_train = pd.concat(train_list)\n",
    "    ratings_test = pd.concat(test_list)\n",
    "    \n",
    "    return ratings_train, ratings_test\n",
    "\n",
    "# Split the ratings data into train and test\n",
    "ratings_train, ratings_test = leave_one_out_split(ratings_df)\n",
    "\n",
    "# Create train sparse matrix\n",
    "train_row_ind = [user_id_map[x] for x in ratings_train['userId']]\n",
    "train_col_ind = [movie_id_map[x] for x in ratings_train['movieId']]\n",
    "train_ratings = ratings_train['rating'].values\n",
    "\n",
    "train_matrix = csr_matrix(\n",
    "    (train_ratings, (train_row_ind, train_col_ind)),\n",
    "    shape=(len(user_ids), len(movie_ids))\n",
    ")\n",
    "\n",
    "# Create test sparse matrix\n",
    "test_row_ind = [user_id_map[x] for x in ratings_test['userId']]\n",
    "test_col_ind = [movie_id_map[x] for x in ratings_test['movieId']]\n",
    "test_ratings = ratings_test['rating'].values\n",
    "\n",
    "test_matrix = csr_matrix(\n",
    "    (test_ratings, (test_row_ind, test_col_ind)),\n",
    "    shape=(len(user_ids), len(movie_ids))\n",
    ")\n",
    "\n",
    "print(\"Training matrix shape:\", train_matrix.shape)\n",
    "print(\"Testing matrix shape:\", test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_movie_features():\n",
    "    # Get unique genres\n",
    "    all_genres = set()\n",
    "    for genres in movies_df['genres'].str.split('|'):\n",
    "        all_genres.update(genres)\n",
    "    \n",
    "    # Create a mapping of genre to index\n",
    "    genre_to_idx = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "    \n",
    "    # Create a feature matrix for movies\n",
    "    movie_features = []\n",
    "    for _, row in movies_df.iterrows():\n",
    "        movie_id = row['movieId']\n",
    "        # Get the TMDB ID for this movie\n",
    "        tmdb_id = links_df[links_df['movieId'] == movie_id]['tmdbId'].values\n",
    "        if len(tmdb_id) > 0:\n",
    "            tmdb_id = tmdb_id[0]\n",
    "            # Create a feature vector for this movie\n",
    "            feature_vector = np.zeros(len(genre_to_idx))\n",
    "            for genre in row['genres'].split('|'):\n",
    "                if genre in genre_to_idx:\n",
    "                    feature_vector[genre_to_idx[genre]] = 1\n",
    "            movie_features.append((tmdb_id, feature_vector))\n",
    "    \n",
    "    return movie_features, genre_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrices\n",
    "movie_features, genre_to_idx = create_movie_features()\n",
    "\n",
    "# Convert to the format LightFM expects\n",
    "item_features = []\n",
    "item_feature_map = {}\n",
    "for tmdb_id, features in movie_features:\n",
    "    if tmdb_id in movie_id_map:\n",
    "        item_idx = movie_id_map[tmdb_id]\n",
    "        item_features.append(features)\n",
    "        item_feature_map[item_idx] = len(item_features) - 1\n",
    "\n",
    "# Convert to sparse matrix\n",
    "item_features = csr_matrix(np.array(item_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, user_id_map, movie_id_map, user_ids, movie_ids, item_features, genre_to_idx, train_matrix, test_matrix, save_dir='models'):\n",
    "    \"\"\"\n",
    "    Save the model and all associated data\n",
    "    \n",
    "    Parameters:\n",
    "    model: The trained LightFM model\n",
    "    user_id_map: Dictionary mapping user IDs to indices\n",
    "    movie_id_map: Dictionary mapping movie IDs to indices\n",
    "    user_ids: Array of user IDs\n",
    "    movie_ids: Array of movie IDs\n",
    "    item_features: Sparse matrix of item features\n",
    "    genre_to_idx: Dictionary mapping genres to indices\n",
    "    train_matrix: Sparse matrix of training data\n",
    "    test_matrix: Sparse matrix of test data\n",
    "    save_dir: Directory to save the model files\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the model and mappings\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'user_id_map': user_id_map,\n",
    "        'movie_id_map': movie_id_map,\n",
    "        'user_ids': user_ids,\n",
    "        'movie_ids': movie_ids,\n",
    "        'genre_to_idx': genre_to_idx\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(os.path.join(save_dir, 'lightfm_model.pkl'), 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    # Save the sparse matrices separately\n",
    "    save_npz(os.path.join(save_dir, 'train_matrix.npz'), train_matrix)\n",
    "    save_npz(os.path.join(save_dir, 'test_matrix.npz'), test_matrix)\n",
    "    save_npz(os.path.join(save_dir, 'item_features.npz'), item_features)\n",
    "    \n",
    "    # Print file sizes\n",
    "    model_size = os.path.getsize(os.path.join(save_dir, 'lightfm_model.pkl')) / (1024 * 1024)  # Convert to MB\n",
    "    train_matrix_size = os.path.getsize(os.path.join(save_dir, 'train_matrix.npz')) / (1024 * 1024)\n",
    "    test_matrix_size = os.path.getsize(os.path.join(save_dir, 'test_matrix.npz')) / (1024 * 1024)\n",
    "    item_features_size = os.path.getsize(os.path.join(save_dir, 'item_features.npz')) / (1024 * 1024)\n",
    "    \n",
    "    print(\"Model and associated data saved successfully!\")\n",
    "\n",
    "def load_model(load_dir='models'):\n",
    "    \"\"\"\n",
    "    Load the saved model and associated data\n",
    "    \n",
    "    Parameters:\n",
    "    load_dir: Directory where the model files are saved\n",
    "    \"\"\"\n",
    "    # Load the sparse matrices\n",
    "    train_matrix = load_npz(os.path.join(load_dir, 'train_matrix.npz'))\n",
    "    test_matrix = load_npz(os.path.join(load_dir, 'test_matrix.npz'))\n",
    "    item_features = load_npz(os.path.join(load_dir, 'item_features.npz'))\n",
    "    \n",
    "    # Load the model and mappings\n",
    "    with open(os.path.join(load_dir, 'lightfm_model.pkl'), 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    return (\n",
    "        model_data['model'],\n",
    "        model_data['user_id_map'],\n",
    "        model_data['movie_id_map'],\n",
    "        model_data['user_ids'],\n",
    "        model_data['movie_ids'],\n",
    "        item_features,\n",
    "        model_data['genre_to_idx'],\n",
    "        train_matrix,\n",
    "        test_matrix\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "model, user_id_map, movie_id_map, user_ids, movie_ids, item_features, genre_to_idx, train_matrix, test_matrix = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 50/50 [39:55<00:00, 47.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and associated data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = LightFM(no_components=30,\n",
    "                loss='warp',\n",
    "                learning_rate=0.05,\n",
    "                item_alpha=1e-6,\n",
    "                user_alpha=1e-6)\n",
    "\n",
    "# Fit the model with item features\n",
    "model.fit(train_matrix,\n",
    "          item_features=item_features,\n",
    "          epochs=30,\n",
    "          num_threads=8,\n",
    "          verbose=True)\n",
    "\n",
    "save_model(\n",
    "    model=model,\n",
    "    user_id_map=user_id_map,\n",
    "    movie_id_map=movie_id_map,\n",
    "    user_ids=user_ids,\n",
    "    movie_ids=movie_ids,\n",
    "    item_features=item_features,\n",
    "    genre_to_idx=genre_to_idx,\n",
    "    train_matrix=train_matrix,\n",
    "    test_matrix=test_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_title(tmdb_id):\n",
    "    try:\n",
    "        movielens_id = links_df[links_df['tmdbId'] == tmdb_id]['movieId'].values[0]\n",
    "        return movies_df[movies_df['movieId'] == movielens_id]['title'].values[0]\n",
    "    except:\n",
    "        return f\"Movie with ID {tmdb_id} (title not found)\"\n",
    "\n",
    "def get_recommendations(user_id, n_items=10):\n",
    "    \"\"\"\n",
    "    Get recommendations for a user\n",
    "    \n",
    "    Parameters:\n",
    "    user_id: The user ID to get recommendations for\n",
    "    n_items: Number of items to recommend\n",
    "    \"\"\"\n",
    "    # Get the internal user index\n",
    "    user_idx = user_id_map[user_id]\n",
    "    \n",
    "    # Get scores for all items\n",
    "    scores = model.predict(user_idx, \n",
    "                         np.arange(len(movie_ids)),\n",
    "                         item_features=item_features)\n",
    "    \n",
    "    # Get the top N items\n",
    "    top_items = np.argsort(-scores)[:n_items]\n",
    "    \n",
    "    # Convert back to original TMDB IDs and verify they exist in our dataset\n",
    "    top_tmdb_ids = []\n",
    "    for idx in top_items:\n",
    "        tmdb_id = list(movie_id_map.keys())[list(movie_id_map.values()).index(idx)]\n",
    "        # Verify the movie exists in our dataset\n",
    "        if tmdb_id in links_df['tmdbId'].values:\n",
    "            top_tmdb_ids.append(tmdb_id)\n",
    "            if len(top_tmdb_ids) >= n_items:\n",
    "                break\n",
    "    \n",
    "    return top_tmdb_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 recommendations for user 1:\n",
      "- Blue Juice (1995)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "user_id = 1  # Replace with actual user ID\n",
    "recommendations = get_recommendations(user_id)\n",
    "print(f\"Top 10 recommendations for user {user_id}:\")\n",
    "for tmdb_id in recommendations:\n",
    "    try:\n",
    "        movie_title = get_movie_title(tmdb_id)\n",
    "        print(f\"- {movie_title}\")\n",
    "    except:\n",
    "        print(f\"- Movie with TMDB ID {tmdb_id} (title not found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New user's initial ratings:\n",
      "- Toy Story (1995): 5.0\n",
      "- Toy Story 2 (1999): 4.5\n",
      "- Toy Story 3 (2010): 5.0\n",
      "- Snow White and the Seven Dwarfs (1937): 5.0\n",
      "- Tangled (2010): 5.0\n",
      "- Tarzan (1999): 5.0\n",
      "\n",
      "Recommended movies for new user:\n",
      "- Blue Juice (1995)\n",
      "- Happy Feet Two (2011)\n",
      "- Match Factory Girl, The (Tulitikkutehtaan tyttö) (1990)\n",
      "- Apartment, The (1960)\n",
      "- Deepwater Horizon (2016)\n",
      "- George Carlin: It's Bad for Ya! (2008)\n",
      "- Walk in the Clouds, A (1995)\n",
      "- Promise, The (La promesse) (1996)\n",
      "- In Too Deep (1999)\n",
      "- My Voyage to Italy (Il mio viaggio in Italia) (1999)\n"
     ]
    }
   ],
   "source": [
    "def handle_new_user(user_ratings, n_recommendations=10):\n",
    "    \"\"\"\n",
    "    Handle a new user by creating a temporary profile and making recommendations\n",
    "    \n",
    "    Parameters:\n",
    "    user_ratings: list of tuples (movie_id, rating) where movie_id is the TMDB ID\n",
    "    n_recommendations: number of recommendations to return\n",
    "    \"\"\"\n",
    "    # Create a new user ID (one more than the maximum existing user ID)\n",
    "    new_user_id = max(user_id_map.keys()) + 1\n",
    "    \n",
    "    # Create a temporary sparse matrix for the new user\n",
    "    new_user_matrix = csr_matrix((1, len(movie_ids)))\n",
    "    \n",
    "    # Add the user's ratings to the matrix\n",
    "    for movie_id, rating in user_ratings:\n",
    "        if movie_id in movie_id_map:\n",
    "            movie_idx = movie_id_map[movie_id]\n",
    "            new_user_matrix[0, movie_idx] = rating\n",
    "    \n",
    "    # Get the model's predictions for the new user\n",
    "    scores = model.predict(0, \n",
    "                         np.arange(len(movie_ids)),\n",
    "                         item_features=item_features)\n",
    "    \n",
    "    # Get the top N items that the user hasn't rated\n",
    "    rated_movies = set(movie_id for movie_id, _ in user_ratings)\n",
    "    top_items = []\n",
    "    for idx in np.argsort(-scores):\n",
    "        movie_id = list(movie_id_map.keys())[list(movie_id_map.values()).index(idx)]\n",
    "        # Verify the movie exists in our dataset and hasn't been rated\n",
    "        if movie_id in links_df['tmdbId'].values and movie_id not in rated_movies:\n",
    "            top_items.append(movie_id)\n",
    "            if len(top_items) >= n_recommendations:\n",
    "                break\n",
    "    return top_items\n",
    "\n",
    "# Example usage:\n",
    "new_user_ratings = [\n",
    "    (862, 5.0),  # Toy Story\n",
    "    (863, 4.5),  # Toy Story 2\n",
    "    (10193, 5.0),  # Toy Story 3\n",
    "    (408, 5.0), # Snow White and the Seven Dwarfs\n",
    "    (38757, 5.0), \n",
    "    (37135, 5.0),\n",
    "]\n",
    "\n",
    "print(\"New user's initial ratings:\")\n",
    "for movie_id, rating in new_user_ratings:\n",
    "    print(f\"- {get_movie_title(movie_id)}: {rating}\")\n",
    "\n",
    "print(\"\\nRecommended movies for new user:\")\n",
    "recommendations = handle_new_user(new_user_ratings)\n",
    "for movie_id in recommendations:\n",
    "    print(f\"- {get_movie_title(movie_id)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Content-based recommendations for genre preferences:\n",
      "- City Hunter (Sing si lip yan) (1993)\n",
      "- Gilda (1946)\n",
      "- Taking of Pelham One Two Three, The (1974)\n",
      "- Grudge, The (2004)\n"
     ]
    }
   ],
   "source": [
    "def get_content_based_recommendations(genre_preferences, n_recommendations=10):\n",
    "    \"\"\"\n",
    "    Get recommendations based on genre preferences for users with no ratings\n",
    "    \n",
    "    Parameters:\n",
    "    genre_preferences: list of preferred genres\n",
    "    n_recommendations: number of recommendations to return\n",
    "    \"\"\"\n",
    "    # Create a feature vector for the genre preferences\n",
    "    feature_vector = np.zeros(len(genre_to_idx))\n",
    "    for genre in genre_preferences:\n",
    "        if genre in genre_to_idx:\n",
    "            feature_vector[genre_to_idx[genre]] = 1\n",
    "    \n",
    "    # Convert to sparse matrix\n",
    "    user_features = csr_matrix(feature_vector.reshape(1, -1))\n",
    "    \n",
    "    # Get scores for all items\n",
    "    scores = model.predict(0, \n",
    "                         np.arange(len(movie_ids)),\n",
    "                         item_features=item_features,\n",
    "                         user_features=user_features)\n",
    "    \n",
    "    # Get the top N items\n",
    "    top_items = np.argsort(-scores)[:n_recommendations]\n",
    "    \n",
    "    # Convert back to original TMDB IDs and verify they exist in our dataset\n",
    "    top_tmdb_ids = []\n",
    "    for idx in top_items:\n",
    "        tmdb_id = list(movie_id_map.keys())[list(movie_id_map.values()).index(idx)]\n",
    "        # Verify the movie exists in our dataset\n",
    "        if tmdb_id in links_df['tmdbId'].values:\n",
    "            top_tmdb_ids.append(tmdb_id)\n",
    "            if len(top_tmdb_ids) >= n_recommendations:\n",
    "                break\n",
    "    \n",
    "    return top_tmdb_ids\n",
    "\n",
    "# Example usage for content-based recommendations:\n",
    "print(\"\\nContent-based recommendations for genre preferences:\")\n",
    "genre_preferences = ['Animation', 'Children']\n",
    "content_recommendations = get_content_based_recommendations(genre_preferences)\n",
    "for movie_id in content_recommendations:\n",
    "    print(f\"- {get_movie_title(movie_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and all associated data\n",
    "model, user_id_map, movie_id_map, user_ids, movie_ids, item_features, genre_to_idx, train_matrix, test_matrix = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train precision: 0.00\n",
      "Test precision: 0.00\n",
      "Train AUC: 0.70\n",
      "Test AUC: 0.58\n"
     ]
    }
   ],
   "source": [
    "# Calculate precision@k and AUC for the test set using a sample of users\n",
    "nonzero_users = test_matrix.getnnz(axis=1).nonzero()[0]\n",
    "sample_users = np.random.choice(nonzero_users, size=100, replace=False)\n",
    "\n",
    "# Create a sparse matrix for user features (identity matrix for users)\n",
    "user_features = identity(train_matrix.shape[0], format='csr')\n",
    "\n",
    "train_precision = precision_at_k(model, \n",
    "                               train_matrix[sample_users], \n",
    "                               k=10,\n",
    "                               item_features=item_features,\n",
    "                               user_features=user_features[sample_users]).mean()\n",
    "print('Train precision: %.2f' % train_precision)\n",
    "\n",
    "test_precision = precision_at_k(model, \n",
    "                              test_matrix[sample_users], \n",
    "                              k=10,\n",
    "                              item_features=item_features,\n",
    "                              user_features=user_features[sample_users]).mean()\n",
    "print('Test precision: %.2f' % test_precision)\n",
    "\n",
    "train_auc = auc_score(model, \n",
    "                     train_matrix[sample_users],\n",
    "                     item_features=item_features,\n",
    "                     user_features=user_features[sample_users]).mean()\n",
    "print('Train AUC: %.2f' % train_auc)\n",
    "\n",
    "test_auc = auc_score(model, \n",
    "                    test_matrix[sample_users],\n",
    "                    item_features=item_features,\n",
    "                    user_features=user_features[sample_users]).mean()\n",
    "print('Test AUC: %.2f' % test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "movies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
