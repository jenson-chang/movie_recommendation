{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<i>Copyright (c) Recommenders contributors.</i>\n",
                "\n",
                "<i>Licensed under the MIT License.</i>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LightFM -  Factorization Machine on MovieLens (Python, CPU)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook explains the concept of a Factorization Machine based model for recommendation, it also outlines the steps to construct a pure matrix factorization and a Factorization Machine using the [LightFM](https://github.com/lyst/lightfm) package. It also demonstrates how to extract both user and item affinity from a fitted model.\n",
                "\n",
                "*NOTE: LightFM is not available in the core package of Recommenders, to run this notebook, install the experimental package with `pip install recommenders[experimental]`.*\n",
                "\n",
                "## 1. Factorization Machine model\n",
                "\n",
                "### 1.1 Background\n",
                "\n",
                "In general, most recommendation models can be divided into two categories:\n",
                "- Content based model,\n",
                "- Collaborative filtering model.\n",
                "\n",
                "The content-based model recommends based on similarity of the items and/or users using their description/metadata/profile. On the other hand, collaborative filtering model (discussion is limited to matrix factorization approach in this notebook) computes the latent factors of the users and items. It works based on the assumption that if a group of people expressed similar opinions on an item, these people would tend to have similar opinions on other items. For further background and detailed explanation between these two approaches, the reader can refer to machine learning literatures [3, 4].\n",
                "\n",
                "The choice between the two models is largely based on the data availability. For example, the collaborative filtering model is usually adopted and effective when sufficient ratings/feedbacks have been recorded for a group of users and items.\n",
                "\n",
                "However, if there is a lack of ratings, content based model can be used provided that the metadata of the users and items are available. This is also the common approach to address the cold-start issues, where there are insufficient historical collaborative interactions available to model new users and/or items.\n",
                "\n",
                "\n",
                "### 1.2 Factorization Machine algorithm\n",
                "\n",
                "In view of the above problems, there have been a number of proposals to address the cold-start issues by combining both content-based and collaborative filtering approaches. The Factorization Machine model is among one of the solutions proposed [1].  \n",
                "\n",
                "In general, most approaches proposed different ways of assessing and/or combining the feature data in conjunction with the collaborative information.\n",
                "\n",
                "### 1.3 LightFM package \n",
                "\n",
                "LightFM is a Python implementation of a Factorization Machine recommendation algorithm for both implicit and explicit feedbacks [1].\n",
                "\n",
                "It is a Factorization Machine model which represents users and items as linear combinations of their content features’ latent factors. The model learns **embeddings or latent representations of the users and items in such a way that it encodes user preferences over items**. These representations produce scores for every item for a given user; items scored highly are more likely to be interesting to the user.\n",
                "\n",
                "The user and item embeddings are estimated for every feature, and these features are then added together to be the final representations for users and items. \n",
                "\n",
                "For example, for user i, the model retrieves the i-th row of the feature matrix to find the features with non-zero weights. The embeddings for these features will then be added together to become the user representation e.g. if user 10 has weight 1 in the 5th column of the user feature matrix, and weight 3 in the 20th column, the user 10’s representation is the sum of embedding for the 5th and the 20th features multiplying their corresponding weights. The representation for each items is computed in the same approach. \n",
                "\n",
                "#### 1.3.1 Modelling approach\n",
                "\n",
                "Let $U$ be the set of users and $I$ be the set of items, and each user can be described by a set of user features $f_{u} \\subset F^{U}$ whilst each items can be described by item features $f_{i} \\subset F^{I}$. Both $F^{U}$ and $F^{I}$ are all the features which fully describe all users and items. \n",
                "\n",
                "The LightFM model operates based binary feedbacks, the ratings will be normalised into two groups. The user-item interaction pairs $(u,i) \\in U\\times I$ are the union of positive (favourable reviews) $S^+$ and negative interactions (negative reviews) $S^-$ for explicit ratings. For implicit feedbacks, these can be the observed and not observed interactions respectively.\n",
                "\n",
                "For each user and item feature, their embeddings are $e_{f}^{U}$ and $e_{f}^{I}$ respectively. Furthermore, each feature is also has a scalar bias term ($b_U^f$ for user and $b_I^f$ for item features). The embedding (latent representation) of user $u$ and item $i$ are the sum of its respective features’ latent vectors:\n",
                "\n",
                "$$ \n",
                "q_{u} = \\sum_{j \\in f_{u}} e_{j}^{U}\n",
                "$$\n",
                "\n",
                "$$\n",
                "p_{i} = \\sum_{j \\in f_{i}} e_{j}^{I}\n",
                "$$\n",
                "\n",
                "Similarly the biases for user $u$ and item $i$ are the sum of its respective bias vectors. These variables capture the variation in behaviour across users and items:\n",
                "\n",
                "$$\n",
                "b_{u} = \\sum_{j \\in f_{u}} b_{j}^{U}\n",
                "$$\n",
                "\n",
                "$$\n",
                "b_{i} = \\sum_{j \\in f_{i}} b_{j}^{I}\n",
                "$$\n",
                "\n",
                "In LightFM, the representation for each user/item is a linear weighted sum of its feature vectors.\n",
                "\n",
                "The prediction for user $u$ and item $i$ can be modelled as sigmoid of the dot product of user and item vectors, adjusted by its feature biases as follows:\n",
                "\n",
                "$$\n",
                "\\hat{r}_{ui} = \\sigma (q_{u} \\cdot p_{i} + b_{u} + b_{i})\n",
                "$$\n",
                "\n",
                "As the LightFM is constructed to predict binary outcomes e.g. $S^+$ and $S^-$, the function $\\sigma()$ is based on the [sigmoid function](https://mathworld.wolfram.com/SigmoidFunction.html). \n",
                "\n",
                "The LightFM algorithm estimates interaction latent vectors and bias for features. For model fitting, the cost function of the model consists of maximising the likelihood of data conditional on the parameters described above using stochastic gradient descent. The likelihood can be expressed as follows:\n",
                "\n",
                "$$\n",
                "L = \\prod_{(u,i) \\in S+}\\hat{r}_{ui} \\times \\prod_{(u,i) \\in S-}1 - \\hat{r}_{ui}\n",
                "$$\n",
                "\n",
                "Note that if the feature latent vectors are not available, the algorithm will behaves like a [logistic matrix factorisation model](http://stanford.edu/~rezab/nips2014workshop/submits/logmat.pdf)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Movie recommender with LightFM using only explicit feedbacks"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Import libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.11.12 | packaged by conda-forge | (main, Apr 10 2025, 22:18:52) [Clang 18.1.8 ]\n",
                        "LightFM version: 1.17\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "import itertools\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import sparse\n",
                "\n",
                "import lightfm\n",
                "from lightfm import LightFM\n",
                "from lightfm.data import Dataset\n",
                "from lightfm import cross_validation\n",
                "\n",
                "print(\"System version: {}\".format(sys.version))\n",
                "print(\"LightFM version: {}\".format(lightfm.__version__))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Defining variables"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# Select MovieLens data size\n",
                "MOVIELENS_DATA_SIZE = '20M'\n",
                "\n",
                "# default number of recommendations\n",
                "K = 10\n",
                "# percentage of data used for testing\n",
                "TEST_PERCENTAGE = 0.25\n",
                "# model learning rate\n",
                "LEARNING_RATE = 0.25\n",
                "# no of latent factors\n",
                "NO_COMPONENTS = 20\n",
                "# no of epochs to fit model\n",
                "NO_EPOCHS = 20\n",
                "# no of threads to fit model\n",
                "NO_THREADS = 32\n",
                "# regularisation for both user and item features\n",
                "ITEM_ALPHA = 1e-6\n",
                "USER_ALPHA = 1e-6\n",
                "\n",
                "# seed for pseudonumber generations\n",
                "SEED = 42"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Retrieve data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(32000204, 4)"
                        ]
                    },
                    "execution_count": 47,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "data = pd.read_csv('data/ratings.csv', header=0, dtype={'userId': 'int32', 'movieId': 'int32', 'rating': 'float32'})\n",
                "data.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Prepare data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Before fitting the LightFM model, we need to create an instance of `Dataset` which holds the interaction matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = Dataset()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `fit` method creates the user/item id mappings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Num users: 200948, num_topics: 84432.\n"
                    ]
                }
            ],
            "source": [
                "dataset.fit(users=data['userId'], \n",
                "            items=data['movieId'])\n",
                "\n",
                "# quick check to determine the number of unique users and items in the data\n",
                "num_users, num_topics = dataset.interactions_shape()\n",
                "print(f'Num users: {num_users}, num_topics: {num_topics}.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next is to build the interaction matrix. The `build_interactions` method returns 2 COO sparse matrices, namely the `interactions` and `weights` matrices."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [],
            "source": [
                "(interactions, weights) = dataset.build_interactions(data.iloc[:, 0:3].values)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "LightLM works slightly differently compared to other packages as it expects the train and test sets to have same dimension. Therefore the conventional train test split will not work.\n",
                "\n",
                "The package has included the `cross_validation.random_train_test_split` method to split the interaction data and splits it into two disjoint training and test sets. \n",
                "\n",
                "However, note that **it does not validate the interactions in the test set to guarantee all items and users have historical interactions in the training set**. Therefore this may result into a partial cold-start problem in the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_interactions, test_interactions = cross_validation.random_train_test_split(\n",
                "    interactions, test_percentage=TEST_PERCENTAGE,\n",
                "    random_state=np.random.RandomState(SEED))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Double check the size of both the train and test sets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 51,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Shape of train interactions: (200948, 84432)\n",
                        "Shape of test interactions: (200948, 84432)\n"
                    ]
                }
            ],
            "source": [
                "print(f\"Shape of train interactions: {train_interactions.shape}\")\n",
                "print(f\"Shape of test interactions: {test_interactions.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.4 Fit the LightFM model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this notebook, the LightFM model will be using the weighted Approximate-Rank Pairwise (WARP) as the loss. Further explanation on the topic can be found [here](https://making.lyst.com/lightfm/docs/examples/warp_loss.html#learning-to-rank-using-the-warp-loss).\n",
                "\n",
                "\n",
                "In general, it maximises the rank of positive examples by repeatedly sampling negative examples until a rank violation has been located. This approach is recommended when only positive interactions are present."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 52,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "model1 = LightFM(loss='warp', no_components=NO_COMPONENTS, \n",
                "                 learning_rate=LEARNING_RATE,                 \n",
                "                 random_state=np.random.RandomState(SEED))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The LightFM model can be fitted with the following code:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 53,
            "metadata": {},
            "outputs": [],
            "source": [
                "model1.fit(interactions=train_interactions,\n",
                "          epochs=NO_EPOCHS);"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Afro Samurai (2007)\n",
                        "Mercy (2016)\n",
                        "EvenHand (2002)\n",
                        "Bobby Jones, Stroke of Genius (2004)\n",
                        "That's Black Entertainment (1990)\n"
                    ]
                }
            ],
            "source": [
                "def recommend_cold_start_user(model, item_id_mapping, liked_items_dict, top_n=10):\n",
                "    \"\"\"\n",
                "    Recommend items for a cold-start user based on known liked items.\n",
                "    liked_items_dict: {movieId: rating, ...}\n",
                "    item_id_mapping: {movieId: internal_id, ...}\n",
                "    \"\"\"\n",
                "    # Get internal IDs for liked items\n",
                "    liked_internal_ids = [\n",
                "        item_id_mapping[movie_id]\n",
                "        for movie_id, rating in liked_items_dict.items()\n",
                "        if rating > 0 and movie_id in item_id_mapping\n",
                "    ]\n",
                "\n",
                "    if not liked_internal_ids:\n",
                "        raise ValueError(\"No liked items provided or movieId not in mapping!\")\n",
                "\n",
                "    # Get item embeddings\n",
                "    item_embeddings = model.item_embeddings\n",
                "\n",
                "    # Average embeddings of liked items\n",
                "    user_embedding = np.mean(item_embeddings[liked_internal_ids], axis=0)\n",
                "\n",
                "    # Score all items\n",
                "    scores = item_embeddings.dot(user_embedding)\n",
                "\n",
                "    # Exclude liked items from recommendations\n",
                "    all_internal_ids = np.array(list(item_id_mapping.values()))\n",
                "    mask = ~np.isin(all_internal_ids, liked_internal_ids)\n",
                "    available_internal_ids = all_internal_ids[mask]\n",
                "    available_scores = scores[available_internal_ids]\n",
                "\n",
                "    # Find top N indices among available items\n",
                "    top_indices_relative = np.argsort(-available_scores)[:top_n]\n",
                "    top_internal_ids = available_internal_ids[top_indices_relative]\n",
                "\n",
                "    # Map internal IDs to external movieIds\n",
                "    reverse_item_id_mapping = {v: k for k, v in item_id_mapping.items()}\n",
                "    recommended_items = [reverse_item_id_mapping[idx] for idx in top_internal_ids]\n",
                "\n",
                "    return recommended_items\n",
                "\n",
                "# Usage:\n",
                "user_id_mapping, user_feature_mapping, item_id_mapping, item_feature_mapping = dataset.mapping()\n",
                "\n",
                "liked_items = {\n",
                "    1: 5.0,        # Toy Story\n",
                "    166461: 5.0,   # Moana (2016)\n",
                "    167036: 5.0,   # Sing (2016)\n",
                "    172547: 5.0,   # Despicable Me 3 (2017)\n",
                "    152081: 5.0,   # Zootopia (2016)\n",
                "    596: 5.0,   # Pinocchio (1940)\n",
                "    170957: 5.0, #Cars 3 (2017)\n",
                "    135887: 5.0, #Minions (2015)\n",
                "    134853: 5.0, #Inside Out (2015)\n",
                "    115617\t: 5.0, #Big Hero 6 (2014)\n",
                "}\n",
                "\n",
                "recommended_items = recommend_cold_start_user(model1, item_id_mapping, liked_items, top_n=5)\n",
                "\n",
                "movies = pd.read_csv('data/movies.csv')\n",
                "movie_id_to_title = dict(zip(movies['movieId'], movies['title']))\n",
                "\n",
                "for item_id in recommended_items:\n",
                "    print(movie_id_to_title.get(item_id, f\"Unknown movie (ID {item_id})\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "movies",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
